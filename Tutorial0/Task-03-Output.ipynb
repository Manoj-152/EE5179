{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 03:\n",
    "\n",
    "In this task, you have to implement the Backpropagation method using Pytorch. This is particularly useful when the hypothesis function contains several weights.\n",
    "\n",
    "**Backpropagation**: Algorithm to caculate gradient for all the weights in the network with several weights. \n",
    "\n",
    "* It uses the `Chain Rule` to calcuate the gradient for multiple nodes at the same time. \n",
    "* In pytorch this is implemented using a `variable` data type and `loss.backward()` method to get the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries - Pytorch Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1 : \n",
    "Convert the following lists into a tensor and analyse it's properties.\n",
    "\n",
    "* Integer List $[0, 1, 2, 3, 4]$ to a $int$ torch tensor\n",
    "* Float List $[0.0, 1.0, 2.0, 3.0, 4.0]$ to a $float$ torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dtype of tensor object after converting it to tensor:  torch.int32\n",
      "The type of tensor object after converting it to tensor:  torch.IntTensor\n",
      "The size of the ints_to_tensor:  torch.Size([4])\n",
      "The dimension of the ints_to_tensor:  1\n"
     ]
    }
   ],
   "source": [
    "int_list = [1,2,3,4]\n",
    "ints_to_tensor = torch.IntTensor(int_list)\n",
    "print(\"The dtype of tensor object after converting it to tensor: \", ints_to_tensor.dtype)\n",
    "print(\"The type of tensor object after converting it to tensor: \", ints_to_tensor.type())\n",
    "print(\"The size of the ints_to_tensor: \", ints_to_tensor.size())\n",
    "print(\"The dimension of the ints_to_tensor: \",ints_to_tensor.ndimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dtype of tensor object after converting it to tensor:  torch.float32\n",
      "The type of tensor object after converting it to tensor:  torch.FloatTensor\n",
      "The size of the floats_to_tensor:  torch.Size([5])\n",
      "The dimension of the floats_to_tensor:  1\n"
     ]
    }
   ],
   "source": [
    "float_list = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "floats_to_tensor = torch.FloatTensor(float_list)\n",
    "print(\"The dtype of tensor object after converting it to tensor: \", floats_to_tensor.dtype)\n",
    "print(\"The type of tensor object after converting it to tensor: \", floats_to_tensor.type())\n",
    "print(\"The size of the floats_to_tensor: \", floats_to_tensor.size())\n",
    "print(\"The dimension of the floats_to_tensor: \",floats_to_tensor.ndimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2 :\n",
    "Convert the given numpy array to a torch tensor and again back to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numpy array:  [[11 12 13]\n",
      " [21 22 23]\n",
      " [31 32 33]]\n",
      "Type :  int64\n",
      "\n",
      "Numpy Array -> Tensor:\n",
      "The tensor after converting: tensor([[11, 12, 13],\n",
      "        [21, 22, 23],\n",
      "        [31, 32, 33]])\n",
      "Type after converting:  torch.int64\n",
      "\n",
      "Tensor -> Numpy Array:\n",
      "The numpy array after converting:  [[11 12 13]\n",
      " [21 22 23]\n",
      " [31 32 33]]\n",
      "Type after converting:  int64\n"
     ]
    }
   ],
   "source": [
    "my_list = [[11, 12, 13], [21, 22, 23], [31, 32, 33]]\n",
    "\n",
    "numpy_array = np.asarray(my_list)\n",
    "print(\"The numpy array: \", numpy_array)\n",
    "print(\"Type : \", numpy_array.dtype)\n",
    "\n",
    "tensor_array = torch.from_numpy(numpy_array)\n",
    "print(\"\\nNumpy Array -> Tensor:\")\n",
    "print(\"The tensor after converting:\", tensor_array)\n",
    "print(\"Type after converting: \", tensor_array.dtype)\n",
    "\n",
    "new_numpy_array = tensor_array.numpy()\n",
    "print(\"\\nTensor -> Numpy Array:\")\n",
    "print(\"The numpy array after converting: \", new_numpy_array)\n",
    "print(\"Type after converting: \", new_numpy_array.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3 :\n",
    "Determine the derivative of $y = 2x^{3} + x$ at $x = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of Y at x=1 :  tensor(3., grad_fn=<AddBackward0>)\n",
      "Derivative of Y wrt x at x=1 :  tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "y = 2 * (x ** 3) + x\n",
    "y.backward()\n",
    "print(\"Value of Y at x=1 : \" , y)\n",
    "print(\"Derivative of Y wrt x at x=1 : \" , x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersie 4 :\n",
    "Determine the partial derivative of $y = uv + u^{2}$ at $u=1$ and $v=2$ with respect to $u$ and $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of y at u=1, v=2 :  tensor(3., grad_fn=<AddBackward0>)\n",
      "Partial Derivative of y wrt u :  tensor(4.)\n",
      "Partial Derivative of y wrt v :  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor(1.0, requires_grad=True)\n",
    "v = torch.tensor(2.0, requires_grad=True)\n",
    "y = (u * v) + (u**2)\n",
    "y.backward()\n",
    "print(\"Value of y at u=1, v=2 : \" , y)\n",
    "print(\"Partial Derivative of y wrt u : \" , u.grad)\n",
    "print(\"Partial Derivative of y wrt v : \" , v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Function and Loss Function\n",
    "\n",
    "$y = x * w + b$\n",
    "\n",
    "$loss =(\\hat{y}-y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make use of a randomly-created sample dataset as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample-dataset\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 03 - a\n",
    "Declare pytorch tensors for weight and bias and implement the forward and loss function of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define w = 1 and b = -1 for y = wx + b\n",
    "# Note that w,b are learnable paramteter \n",
    "# i.e., you are going to take the derivative of the tensor(s).\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "b = torch.tensor([-1.0], requires_grad=True)\n",
    "\n",
    "assert w.item() == 1\n",
    "assert b.item() == -1\n",
    "assert w.requires_grad == True\n",
    "assert b.requires_grad == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for forward pass to predict y\n",
    "def forward(x):\n",
    "    return x*w + b\n",
    "\n",
    "# Function to calcuate the loss of the model\n",
    "# Loss is the square of difference of prediction and actual value\n",
    "def loss(y_pred,y_actual):\n",
    "    return (y_pred-y_actual)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $y_{pred}$ for $x=4$ without training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_without_train = forward(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 8.32815933227539 | w: 1.368575930595398\n",
      "Epoch: 2 | Loss: 4.635132312774658 | w: 1.641068696975708\n",
      "Epoch: 3 | Loss: 2.6127521991729736 | w: 1.842525839805603\n",
      "Epoch: 4 | Loss: 1.5045195817947388 | w: 1.991465449333191\n",
      "Epoch: 5 | Loss: 0.8966817855834961 | w: 2.1015784740448\n",
      "Epoch: 6 | Loss: 0.5628984570503235 | w: 2.182986259460449\n",
      "Epoch: 7 | Loss: 0.3793121576309204 | w: 2.2431719303131104\n",
      "Epoch: 8 | Loss: 0.2781200110912323 | w: 2.287667751312256\n",
      "Epoch: 9 | Loss: 0.22218374907970428 | w: 2.3205642700195312\n",
      "Epoch: 10 | Loss: 0.19114667177200317 | w: 2.3448848724365234\n"
     ]
    }
   ],
   "source": [
    "# In this method, we learn the dataset multiple times (called epochs)\n",
    "# Each time, the weight (w) gets updates using the graident decent algorithm based on weights of the previous epoch\n",
    "\n",
    "alpha = 0.01 # Let us set learning rate as 0.01\n",
    "weight_list = []\n",
    "loss_list=[]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for x, y in zip(x_data, y_data):\n",
    "        \n",
    "        y_pred = forward(x) # Forward pass\n",
    "        current_loss = loss(y_pred,y) # Loss\n",
    "        total_loss += current_loss\n",
    "        current_loss.backward() # Backpropagation\n",
    "        \n",
    "        w.data = w.data - alpha * w.grad.item() # update weights\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    avg_mse = total_loss / count        \n",
    "    print(f\"Epoch: {epoch+1} | Loss: {avg_mse.item()} | w: {w.item()}\")\n",
    "    weight_list.append(w)\n",
    "    loss_list.append(avg_mse)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $y_{pred}$ for $x=4$ after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Y Value for x=4 : 8\n",
      "Predicted Y Value before training :  3.0\n",
      "Predicted Y Value after training :  8.379539489746094\n"
     ]
    }
   ],
   "source": [
    "y_pred_with_train = forward(4)\n",
    "\n",
    "print(\"Actual Y Value for x=4 : 8\")\n",
    "print(\"Predicted Y Value before training : \" , y_pred_without_train.item())\n",
    "print(\"Predicted Y Value after training : \" , y_pred_with_train.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 03 - b\n",
    "Repeat **Task:03 - a** for the quadratic model defined below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using backward propagation for quadratic model\n",
    "\n",
    "$\\hat{y} = x^2*w_{2} + x*w_{1}$\n",
    "\n",
    "$loss = (\\hat{y}-y)^2$\n",
    "\n",
    "* Using Dummy values of x and y\n",
    "\n",
    "`x = 1,2,3,4,5`\n",
    "`y = 1,6,15,28,45`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "y_data = [1.0, 6.0, 15.0, 28, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the given dataset\n",
    "plt.plot(x_data,y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize w2 and w1 with randon values\n",
    "w_1 = torch.tensor([1.0], requires_grad=True)\n",
    "w_2 = torch.tensor([1.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic forward pass based on the function above. Taking b as zero for now\n",
    "def quad_forward(x):\n",
    "    return w_1*(x**2)+w_2*x\n",
    "\n",
    "# Loss fucntion as per the defination above\n",
    "def loss(y_pred,y_actual):\n",
    "    return (y_pred-y_actual)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $y_{pred}$ for $x=6$ without training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_without_train = quad_forward(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 19.670841217041016 | w1: 1.7099769115447998 | w2: 1.1621068716049194\n",
      "Epoch: 2 | Loss: 6.430711269378662 | w1: 1.610548496246338 | w2: 1.115532636642456\n",
      "Epoch: 3 | Loss: 4.341702938079834 | w1: 1.629080057144165 | w2: 1.099595308303833\n",
      "Epoch: 4 | Loss: 4.463932037353516 | w1: 1.6303775310516357 | w2: 1.0794039964675903\n",
      "Epoch: 5 | Loss: 4.349801063537598 | w1: 1.6341499090194702 | w2: 1.060043454170227\n",
      "Epoch: 6 | Loss: 4.273285865783691 | w1: 1.6375246047973633 | w2: 1.0407702922821045\n",
      "Epoch: 7 | Loss: 4.193112373352051 | w1: 1.6409205198287964 | w2: 1.0216909646987915\n",
      "Epoch: 8 | Loss: 4.115159511566162 | w1: 1.644276738166809 | w2: 1.0027880668640137\n",
      "Epoch: 9 | Loss: 4.038552284240723 | w1: 1.6476027965545654 | w2: 0.9840622544288635\n",
      "Epoch: 10 | Loss: 3.9633827209472656 | w1: 1.65089750289917 | w2: 0.9655115008354187\n",
      "Epoch: 11 | Loss: 3.8896148204803467 | w1: 1.654161810874939 | w2: 0.9471341967582703\n",
      "Epoch: 12 | Loss: 3.817220687866211 | w1: 1.6573951244354248 | w2: 0.928928554058075\n",
      "Epoch: 13 | Loss: 3.7461726665496826 | w1: 1.660598635673523 | w2: 0.9108932614326477\n",
      "Epoch: 14 | Loss: 3.6764445304870605 | w1: 1.6637717485427856 | w2: 0.8930264711380005\n",
      "Epoch: 15 | Loss: 3.6080145835876465 | w1: 1.6669154167175293 | w2: 0.8753268718719482\n",
      "Epoch: 16 | Loss: 3.5408616065979004 | w1: 1.670029878616333 | w2: 0.8577927947044373\n",
      "Epoch: 17 | Loss: 3.4749622344970703 | w1: 1.6731152534484863 | w2: 0.8404226303100586\n",
      "Epoch: 18 | Loss: 3.4102847576141357 | w1: 1.6761715412139893 | w2: 0.8232148885726929\n",
      "Epoch: 19 | Loss: 3.3468120098114014 | w1: 1.6791993379592896 | w2: 0.8061680793762207\n",
      "Epoch: 20 | Loss: 3.2845165729522705 | w1: 1.6821985244750977 | w2: 0.7892804741859436\n",
      "Epoch: 21 | Loss: 3.2233805656433105 | w1: 1.6851699352264404 | w2: 0.7725509405136108\n",
      "Epoch: 22 | Loss: 3.163391590118408 | w1: 1.688113808631897 | w2: 0.7559778690338135\n",
      "Epoch: 23 | Loss: 3.1045126914978027 | w1: 1.6910297870635986 | w2: 0.7395595908164978\n",
      "Epoch: 24 | Loss: 3.04672908782959 | w1: 1.6939187049865723 | w2: 0.7232949137687683\n",
      "Epoch: 25 | Loss: 2.990025758743286 | w1: 1.6967805624008179 | w2: 0.7071822285652161\n",
      "Epoch: 26 | Loss: 2.934373140335083 | w1: 1.6996155977249146 | w2: 0.6912202835083008\n",
      "Epoch: 27 | Loss: 2.8797547817230225 | w1: 1.7024239301681519 | w2: 0.6754074692726135\n",
      "Epoch: 28 | Loss: 2.8261539936065674 | w1: 1.7052063941955566 | w2: 0.659742534160614\n",
      "Epoch: 29 | Loss: 2.773554563522339 | w1: 1.7079628705978394 | w2: 0.6442241668701172\n",
      "Epoch: 30 | Loss: 2.7219345569610596 | w1: 1.7106932401657104 | w2: 0.6288508772850037\n",
      "Epoch: 31 | Loss: 2.6712708473205566 | w1: 1.7133980989456177 | w2: 0.6136212944984436\n",
      "Epoch: 32 | Loss: 2.621551990509033 | w1: 1.7160779237747192 | w2: 0.5985341668128967\n",
      "Epoch: 33 | Loss: 2.572758436203003 | w1: 1.7187325954437256 | w2: 0.5835880637168884\n",
      "Epoch: 34 | Loss: 2.5248758792877197 | w1: 1.7213623523712158 | w2: 0.5687816739082336\n",
      "Epoch: 35 | Loss: 2.4778826236724854 | w1: 1.7239676713943481 | w2: 0.5541138052940369\n",
      "Epoch: 36 | Loss: 2.4317634105682373 | w1: 1.7265485525131226 | w2: 0.5395830273628235\n",
      "Epoch: 37 | Loss: 2.3865020275115967 | w1: 1.7291052341461182 | w2: 0.5251880884170532\n",
      "Epoch: 38 | Loss: 2.342083692550659 | w1: 1.7316380739212036 | w2: 0.510927677154541\n",
      "Epoch: 39 | Loss: 2.298491954803467 | w1: 1.7341471910476685 | w2: 0.4968006908893585\n",
      "Epoch: 40 | Loss: 2.2557120323181152 | w1: 1.736633062362671 | w2: 0.4828057885169983\n",
      "Epoch: 41 | Loss: 2.2137269973754883 | w1: 1.7390953302383423 | w2: 0.46894168853759766\n",
      "Epoch: 42 | Loss: 2.1725239753723145 | w1: 1.74153470993042 | w2: 0.4552072286605835\n",
      "Epoch: 43 | Loss: 2.1320881843566895 | w1: 1.743951439857483 | w2: 0.4416012763977051\n",
      "Epoch: 44 | Loss: 2.092405319213867 | w1: 1.7463454008102417 | w2: 0.4281224012374878\n",
      "Epoch: 45 | Loss: 2.053462028503418 | w1: 1.7487173080444336 | w2: 0.41476961970329285\n",
      "Epoch: 46 | Loss: 2.0152411460876465 | w1: 1.7510664463043213 | w2: 0.4015416204929352\n",
      "Epoch: 47 | Loss: 1.977731704711914 | w1: 1.7533941268920898 | w2: 0.3884373605251312\n",
      "Epoch: 48 | Loss: 1.9409242868423462 | w1: 1.755699872970581 | w2: 0.3754555881023407\n",
      "Epoch: 49 | Loss: 1.9047966003417969 | w1: 1.757983922958374 | w2: 0.36259517073631287\n",
      "Epoch: 50 | Loss: 1.869340181350708 | w1: 1.7602466344833374 | w2: 0.34985506534576416\n",
      "Epoch: 51 | Loss: 1.8345476388931274 | w1: 1.7624883651733398 | w2: 0.3372340798377991\n",
      "Epoch: 52 | Loss: 1.8004045486450195 | w1: 1.764709234237671 | w2: 0.32473108172416687\n",
      "Epoch: 53 | Loss: 1.766897201538086 | w1: 1.766909122467041 | w2: 0.3123449683189392\n",
      "Epoch: 54 | Loss: 1.7340103387832642 | w1: 1.7690885066986084 | w2: 0.3000746965408325\n",
      "Epoch: 55 | Loss: 1.701735258102417 | w1: 1.7712475061416626 | w2: 0.28791916370391846\n",
      "Epoch: 56 | Loss: 1.6700595617294312 | w1: 1.7733861207962036 | w2: 0.2758772373199463\n",
      "Epoch: 57 | Loss: 1.6389776468276978 | w1: 1.7755051851272583 | w2: 0.2639479637145996\n",
      "Epoch: 58 | Loss: 1.6084731817245483 | w1: 1.777604103088379 | w2: 0.2521301805973053\n",
      "Epoch: 59 | Loss: 1.578535795211792 | w1: 1.7796835899353027 | w2: 0.24042290449142456\n",
      "Epoch: 60 | Loss: 1.549156665802002 | w1: 1.7817434072494507 | w2: 0.22882501780986786\n",
      "Epoch: 61 | Loss: 1.520322322845459 | w1: 1.78378427028656 | w2: 0.21733567118644714\n",
      "Epoch: 62 | Loss: 1.492027997970581 | w1: 1.7858058214187622 | w2: 0.2059536725282669\n",
      "Epoch: 63 | Loss: 1.4642534255981445 | w1: 1.7878084182739258 | w2: 0.19467811286449432\n",
      "Epoch: 64 | Loss: 1.4370014667510986 | w1: 1.7897924184799194 | w2: 0.1835079789161682\n",
      "Epoch: 65 | Loss: 1.4102556705474854 | w1: 1.7917578220367432 | w2: 0.17244228720664978\n",
      "Epoch: 66 | Loss: 1.3840065002441406 | w1: 1.7937047481536865 | w2: 0.16148003935813904\n",
      "Epoch: 67 | Loss: 1.358245849609375 | w1: 1.7956336736679077 | w2: 0.15062032639980316\n",
      "Epoch: 68 | Loss: 1.3329664468765259 | w1: 1.7975444793701172 | w2: 0.13986217975616455\n",
      "Epoch: 69 | Loss: 1.3081586360931396 | w1: 1.7994375228881836 | w2: 0.12920458614826202\n",
      "Epoch: 70 | Loss: 1.2838106155395508 | w1: 1.8013126850128174 | w2: 0.11864663660526276\n",
      "Epoch: 71 | Loss: 1.2599154710769653 | w1: 1.8031704425811768 | w2: 0.10818740725517273\n",
      "Epoch: 72 | Loss: 1.2364667654037476 | w1: 1.8050107955932617 | w2: 0.09782599657773972\n",
      "Epoch: 73 | Loss: 1.2134513854980469 | w1: 1.8068337440490723 | w2: 0.08756142854690552\n",
      "Epoch: 74 | Loss: 1.190863847732544 | w1: 1.8086398839950562 | w2: 0.07739287614822388\n",
      "Epoch: 75 | Loss: 1.1687015295028687 | w1: 1.8104290962219238 | w2: 0.067319355905056\n",
      "Epoch: 76 | Loss: 1.1469495296478271 | w1: 1.8122016191482544 | w2: 0.05734003335237503\n",
      "Epoch: 77 | Loss: 1.1256020069122314 | w1: 1.8139575719833374 | w2: 0.04745401814579964\n",
      "Epoch: 78 | Loss: 1.1046502590179443 | w1: 1.8156967163085938 | w2: 0.0376603789627552\n",
      "Epoch: 79 | Loss: 1.0840911865234375 | w1: 1.817420244216919 | w2: 0.02795841544866562\n",
      "Epoch: 80 | Loss: 1.0639140605926514 | w1: 1.8191272020339966 | w2: 0.018347082659602165\n",
      "Epoch: 81 | Loss: 1.0441125631332397 | w1: 1.820818543434143 | w2: 0.008825712837278843\n",
      "Epoch: 82 | Loss: 1.0246798992156982 | w1: 1.8224937915802002 | w2: -0.0006067296490073204\n",
      "Epoch: 83 | Loss: 1.0056073665618896 | w1: 1.8241534233093262 | w2: -0.009950952604413033\n",
      "Epoch: 84 | Loss: 0.9868906736373901 | w1: 1.8257975578308105 | w2: -0.019207805395126343\n",
      "Epoch: 85 | Loss: 0.9685209393501282 | w1: 1.8274261951446533 | w2: -0.028378114104270935\n",
      "Epoch: 86 | Loss: 0.9504930377006531 | w1: 1.8290398120880127 | w2: -0.03746265918016434\n",
      "Epoch: 87 | Loss: 0.9328028559684753 | w1: 1.8306381702423096 | w2: -0.046462297439575195\n",
      "Epoch: 88 | Loss: 0.9154413342475891 | w1: 1.8322217464447021 | w2: -0.05537775158882141\n",
      "Epoch: 89 | Loss: 0.8984050750732422 | w1: 1.83379065990448 | w2: -0.06420981138944626\n",
      "Epoch: 90 | Loss: 0.8816840052604675 | w1: 1.8353445529937744 | w2: -0.07295937091112137\n",
      "Epoch: 91 | Loss: 0.8652714490890503 | w1: 1.8368840217590332 | w2: -0.08162709325551987\n",
      "Epoch: 92 | Loss: 0.8491684198379517 | w1: 1.838409185409546 | w2: -0.09021376073360443\n",
      "Epoch: 93 | Loss: 0.8333638906478882 | w1: 1.8399202823638916 | w2: -0.09872013330459595\n",
      "Epoch: 94 | Loss: 0.8178545832633972 | w1: 1.8414167165756226 | w2: -0.10714709758758545\n",
      "Epoch: 95 | Loss: 0.802628219127655 | w1: 1.8428994417190552 | w2: -0.11549517512321472\n",
      "Epoch: 96 | Loss: 0.7876907587051392 | w1: 1.8443684577941895 | w2: -0.12376518547534943\n",
      "Epoch: 97 | Loss: 0.773030161857605 | w1: 1.8458234071731567 | w2: -0.13195794820785522\n",
      "Epoch: 98 | Loss: 0.7586407661437988 | w1: 1.8472650051116943 | w2: -0.14007404446601868\n",
      "Epoch: 99 | Loss: 0.7445210218429565 | w1: 1.8486931324005127 | w2: -0.14811421930789948\n",
      "Epoch: 100 | Loss: 0.7306647300720215 | w1: 1.850108027458191 | w2: -0.15607918798923492\n"
     ]
    }
   ],
   "source": [
    "# In this method, we learn the dataset multiple times (called epochs)\n",
    "# Each time, the weight (w) gets updates using the graident decent algorithm based on weights of the previous epoch\n",
    "\n",
    "alpha = 0.0012 # Let us set learning rate as 0.01\n",
    "weight_list = []\n",
    "loss_list=[]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for x, y in zip(x_data, y_data):\n",
    "        \n",
    "        y_pred = quad_forward(x)\n",
    "        current_loss = loss(y_pred, y)\n",
    "        total_loss += current_loss\n",
    "        current_loss.backward()\n",
    "        \n",
    "        w_1.data = w_1.data - alpha * w_1.grad.item()\n",
    "        w_2.data = w_2.data - alpha * w_2.grad.item()\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w_1.grad.data.zero_()\n",
    "        w_2.grad.data.zero_()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    avg_mse = total_loss / count        \n",
    "    print(f\"Epoch: {epoch+1} | Loss: {avg_mse.item()} | w1: {w_1.item()} | w2: {w_2.item()}\")\n",
    "    weight_list.append(w)\n",
    "    loss_list.append(avg_mse)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $y_{pred}$ for $x=4$ after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Y Value for x=4 : 66\n",
      "Predicted Y Value before training :  42.0\n",
      "Predicted Y Value after training :  65.66741180419922\n"
     ]
    }
   ],
   "source": [
    "y_pred_with_train = quad_forward(6)\n",
    "\n",
    "print(\"Actual Y Value for x=4 : 66\")\n",
    "print(\"Predicted Y Value before training : \" , y_pred_without_train.item())\n",
    "print(\"Predicted Y Value after training : \" , y_pred_with_train.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
